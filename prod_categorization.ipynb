{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "from sklearn import preprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input file and sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    \n",
    "    with open(file_name, encoding='utf-8') as data_file:\n",
    "        data = json.loads(data_file.read())\n",
    "\n",
    "    pprint(data[0])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preprocess the text to remove html tags, special characters, stop words, lemmatization\n",
    "    Save the output to a json'''\n",
    "def preprocess(data):\n",
    "\n",
    "    stopword = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    stopword = list(stopword) +  ['entire', 'collection', 'selection','select', 'free', 'shipping', 'available', \n",
    "                                 'jcrew', 'urban', 'outfitter', 'shopify', 'long', 'longer', 'online'\n",
    "                                 'fit', 'loose','item', 'import', 'shop', 'jcrewcom', 'see', 'body'\n",
    "                                 'price', 'yet', 'youll', 'would', 'could', 'look', 'cotton', 'silk', \n",
    "                                 'tall', 'short', 'petite', 'men', 'size', 'made', 'style', 'size', \n",
    "                                 'all', 'woman', 'man', 'kid', 'girl', 'boy', 'length', 'machine', 'wash',\n",
    "                                 'fit', 'dry', 'clean', 'back', 'body', 'one', 'knit', 'order',\n",
    "                                 'special', 'charge', 'sale', 'dont', 'color'] \n",
    "\n",
    "    corpus = []\n",
    "    for cnt, element in enumerate(data):\n",
    "\n",
    "            cleaned_des = re.sub('<[^<]+?>','', element['description'])\n",
    "            des = re.sub('[^a-z\\-\\s]+', '',cleaned_des.lower())\n",
    "            des = re.sub('[\\-]', ' ',des)\n",
    "            word_tokens = nltk.word_tokenize(des)\n",
    "            lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "            #stemmed_word = [snowball_stemmer.stem(word) for word in lemmatized_word]\n",
    "            removing_stopwords = [word for word in lemmatized_word if word not in set(stopword)]\n",
    "            element['description'] = [word for word in removing_stopwords if len(word) > 2]\n",
    "\n",
    "            element['id'] = cnt\n",
    "            element['Category'] = 'Other'\n",
    "            if len(element['description']) > 1:\n",
    "                corpus.append(' '.join(element['description']))\n",
    "\n",
    "    with open('prepr_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print('Preprocessed data output at prepr_data.json ')\n",
    "               \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf vec generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(corpus, max_df = 0.80, min_df = 0.002):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, min_df = min_df)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(X.shape)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans with cosine distance similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, num_centers = 8):\n",
    "\n",
    "    kclusterer = KMeansClusterer(num_means = num_centers, distance=nltk.cluster.util.cosine_distance,\n",
    "                                 repeats= 7, avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(np.asarray(X.todense()), assign_clusters=True)\n",
    "    \n",
    "    return assigned_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Post-process and get top words in cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(data, assigned_clusters):\n",
    "\n",
    "    track_corpus = 0\n",
    "    corpus_set = {}\n",
    "    \n",
    "    for element in data:\n",
    "        if len(element['description']) > 1:\n",
    "            element['Category'] = assigned_clusters[track_corpus]\n",
    "            corpus_set.setdefault(assigned_clusters[track_corpus],[])\n",
    "            corpus_set[assigned_clusters[track_corpus]].extend(element['description'])\n",
    "            track_corpus = track_corpus + 1\n",
    "            \n",
    "    print(\"Unnamed categories in initial_res_data.json\\n\\n\")\n",
    "    \n",
    "    with open('initial_res_data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    \n",
    "    for key,val in corpus_set.items():\n",
    "        cnt = Counter(val)\n",
    "        print(key)\n",
    "        print(cnt.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save first stage clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_first_stage_res(data, catg):\n",
    "    \n",
    "    for element in data:\n",
    "        if len(element['description']) > 1:\n",
    "            key = int(element['Category'])\n",
    "            element['first_stage'] = catg[key]\n",
    "    \n",
    "    with open('first_stage_res.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    \n",
    "    print(\"Saved first stage clustering results in first_stage_res.json \")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Supersoft speckled French terry makes this (tush covering!) '\n",
      "                \"turtleneck-sweatshirt hybrid the layering piece you'll want \"\n",
      "                'to wear to the gym, to lunch, to, well, everywhere this '\n",
      "                'winter. Loose fit. Body length: 27 1/2. Cotton. Import.',\n",
      " 'images_url': 'https://www.jcrew.com/s7-img-facade/H3588_PK6317_m?fmt=jpeg&qlt=90,0&resMode=sharp&op_usm=.1,0,0,0&wid=200&hei=200'}\n",
      "Preprocessed data output at prepr_data.json \n",
      "(949, 1120)\n"
     ]
    }
   ],
   "source": [
    "file_name = 'product_data.json'\n",
    "max_df = 0.8\n",
    "min_df = 0.003\n",
    "data = read_file(file_name)\n",
    "corpus = preprocess(data) \n",
    "Xvect = tfidf_vectorizer(corpus, max_df = max_df, min_df = min_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramya/miniconda3/lib/python3.6/site-packages/nltk/cluster/util.py:133: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    }
   ],
   "source": [
    "# num_centers were tried from 6-11 and set to 8 (max_categories in product list given)\n",
    "num_centers = 8\n",
    "assigned_clusters = kmeans(Xvect,num_centers)\n",
    "#Save assigned clusters to text file\n",
    "np.savetxt(\"initial_kmeans.out\",assigned_clusters,fmt = '%u', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed categories in initial_res_data.json\n",
      "\n",
      "\n",
      "5.0\n",
      "[('sweater', 22), ('sweatshirt', 15), ('terry', 14), ('french', 13), ('fleece', 11), ('crewneck', 7), ('cashmere', 7), ('cardigan', 6), ('soft', 6), ('hoodie', 6), ('citizen', 5), ('cottonmachine', 5), ('coldtumble', 5), ('lowdo', 5), ('bleach', 5)]\n",
      "0.0\n",
      "[('shirt', 207), ('button', 55), ('sleeve', 35), ('top', 30), ('perfect', 27), ('slim', 20), ('popover', 18), ('stretch', 14), ('stripe', 14), ('classic', 13), ('linen', 11), ('tee', 11), ('ruffle', 11), ('dyed', 9), ('pocket', 9)]\n",
      "1.0\n",
      "[('swimwear', 67), ('top', 64), ('bikini', 39), ('stripe', 28), ('tie', 27), ('bottom', 18), ('print', 16), ('piece', 15), ('swimsuit', 14), ('tank', 14), ('playa', 11), ('shoulder', 10), ('front', 9), ('underwire', 9), ('pajama', 9)]\n",
      "6.0\n",
      "[('dress', 122), ('skirt', 17), ('tie', 16), ('perfect', 14), ('floral', 14), ('sheath', 14), ('ruffle', 12), ('sleeve', 12), ('waist', 12), ('wrap', 11), ('fall', 11), ('mercantile', 10), ('knee', 10), ('flattering', 9), ('sandal', 9)]\n",
      "7.0\n",
      "[('rise', 74), ('hand', 72), ('leg', 62), ('oil', 62), ('soft', 45), ('care', 44), ('detail', 44), ('front', 43), ('extract', 41), ('skin', 40), ('piece', 40), ('fabric', 37), ('content', 35), ('feature', 32), ('perfect', 30)]\n",
      "4.0\n",
      "[('pant', 74), ('jacket', 43), ('und', 36), ('pour', 35), ('avec', 33), ('contenu', 25), ('soins', 24), ('taille', 23), ('mit', 20), ('stretch', 19), ('vous', 18), ('sur', 18), ('une', 17), ('inhalt', 17), ('pflege', 17)]\n",
      "2.0\n",
      "[('earring', 45), ('necklace', 20), ('jewelry', 18), ('fine', 17), ('gold', 17), ('plated', 17), ('demi', 15), ('hoop', 10), ('bead', 10), ('bracelet', 9), ('chain', 9), ('brass', 9), ('closure', 7), ('pearl', 7), ('ring', 6)]\n",
      "3.0\n",
      "[('leather', 41), ('bag', 30), ('accessory', 20), ('heel', 15), ('sole', 14), ('upper', 13), ('lining', 13), ('italian', 12), ('tote', 9), ('synthetic', 9), ('italy', 7), ('strap', 7), ('closure', 7), ('true', 7), ('measurement', 7)]\n"
     ]
    }
   ],
   "source": [
    "clusters = np.loadtxt(\"initial_kmeans.out\")\n",
    "post_process(data, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_names = {5: \"Tops\", 0: \"Tops\", 1 : \"Swimwear, Intimates, Others\", \n",
    "              6: \"Dresses, Skirts, Others\", 7: \"Unnamed, Others\", \n",
    "              4: \"Pant, Jacket, Others\", 2: \"Jewellery\", 3: \"Bags, Shoes\"}\n",
    "save_first_stage_res(data, catg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt second stage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Obtain Sub-corpus of given category'''\n",
    "\n",
    "def get_subcorpus(data, catg):\n",
    "    \n",
    "    corpus = []\n",
    "    for element in data:\n",
    "        if (len(element['description']) > 1 and int(element['Category']) == catg):\n",
    "            corpus.append(' '.join(element['description']))\n",
    "            \n",
    "    return corpus        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Post process second stage clustering results'''\n",
    "\n",
    "def pp_sec_stage(data, catg, assigned_clusters):\n",
    "\n",
    "    track_corpus = 0\n",
    "    corpus_set = {}\n",
    "    \n",
    "    for element in data:\n",
    "        if (len(element['description']) > 1 and int(element['Category']) == catg):\n",
    "            element['Category_Sec'] = assigned_clusters[track_corpus]\n",
    "            corpus_set.setdefault(assigned_clusters[track_corpus],[])\n",
    "            corpus_set[assigned_clusters[track_corpus]].extend(element['description'])\n",
    "            track_corpus = track_corpus + 1\n",
    "            \n",
    "    for key,val in corpus_set.items():\n",
    "        cnt = Counter(val)\n",
    "        print(key)\n",
    "        print(cnt.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Update data based on second stage clustering results'''\n",
    "\n",
    "def update_data(data, catg, catg_names):\n",
    "    \n",
    "    for element in data:\n",
    "        if (len(element['description']) > 1 and int(element['Category']) == catg):\n",
    "            key = int(element['Category_Sec'])\n",
    "            element['sec_stage'] = catg_names[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save second stage clustering results '''\n",
    "\n",
    "def save_sec_stage_res(data):\n",
    "    \n",
    "    with open('final_res.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(\"Saved second stage clustering results in final_res.json \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_stage_res.json', encoding='utf-8') as data_file:\n",
    "        data = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt second stage for Catgeory 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 350)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_df = 0.95\n",
    "min_df = 0.001\n",
    "catg = 1\n",
    "num_centers = 4\n",
    "sub_corpus = get_subcorpus(data, catg ) \n",
    "sub_Xvect = tfidf_vectorizer(sub_corpus, max_df = max_df, min_df = min_df)\n",
    "assigned_clusters = kmeans(sub_Xvect,num_centers)\n",
    "#Save assigned clusters to text file\n",
    "out_filename = \"subkmeans_catg_\" + str(catg)\n",
    "np.savetxt(out_filename,assigned_clusters,fmt = '%u', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[('top', 34), ('tie', 17), ('tank', 14), ('perfect', 7), ('front', 6), ('active', 6), ('pretty', 6), ('shoulder', 5), ('jersey', 5), ('new', 5), ('wrap', 4), ('drapey', 4), ('online', 4), ('slim', 4), ('supimareg', 3)]\n",
      "0.0\n",
      "[('swimwear', 28), ('piece', 14), ('swimsuit', 14), ('stripe', 10), ('print', 7), ('inch', 7), ('cropped', 6), ('ruched', 5), ('rash', 5), ('guard', 5), ('playa', 4), ('scoopback', 4), ('nautical', 4), ('floral', 4), ('board', 4)]\n",
      "2.0\n",
      "[('bikini', 39), ('swimwear', 36), ('top', 23), ('bottom', 18), ('stripe', 9), ('playa', 7), ('tie', 6), ('french', 5), ('seersucker', 5), ('underwire', 5), ('shoulder', 4), ('rickrack', 4), ('libertyreg', 4), ('high', 4), ('floral', 4)]\n",
      "3.0\n",
      "[('pajama', 9), ('set', 8), ('sleeve', 8), ('top', 7), ('stripe', 6), ('print', 6), ('button', 6), ('loungewear', 5), ('online', 5), ('cuff', 4), ('keyhole', 4), ('closure', 4), ('jcew', 3), ('lounge', 3), ('classic', 3)]\n"
     ]
    }
   ],
   "source": [
    "catg = 1\n",
    "out_filename = \"subkmeans_catg_\" + str(catg)\n",
    "clusters = np.loadtxt(out_filename)\n",
    "pp_sec_stage(data, catg, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_names = {1: \"Tops\", 3: \"Others\", 0 : \"Swimwear\", 2 : \"Swimwear\"}\n",
    "update_data(data, catg, catg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt second stage for Catgeory 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 342)\n"
     ]
    }
   ],
   "source": [
    "max_df = 0.95\n",
    "min_df = 0.001\n",
    "catg = 6\n",
    "num_centers = 3\n",
    "sub_corpus = get_subcorpus(data, catg ) \n",
    "sub_Xvect = tfidf_vectorizer(sub_corpus, max_df = max_df, min_df = min_df)\n",
    "assigned_clusters = kmeans(sub_Xvect,num_centers)\n",
    "#Save assigned clusters to text file\n",
    "out_filename = \"subkmeans_catg_\" + str(catg)\n",
    "np.savetxt(out_filename,assigned_clusters,fmt = '%u', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[('dress', 101), ('floral', 12), ('sleeve', 12), ('sheath', 11), ('mercantile', 10), ('wrap', 9), ('fall', 7), ('knee', 7), ('flutter', 5), ('fabric', 5), ('midi', 4), ('portfolio', 4), ('perfect', 4), ('pretty', 4), ('tweed', 3)]\n",
      "2.0\n",
      "[('dress', 20), ('tie', 15), ('perfect', 10), ('waist', 9), ('shoulder', 8), ('linen', 7), ('ruffle', 6), ('flattering', 6), ('high', 6), ('neckline', 6), ('silhouette', 5), ('super', 5), ('mini', 5), ('summer', 5), ('slip', 5)]\n",
      "0.0\n",
      "[('skirt', 16), ('sandal', 6), ('ruffle', 3), ('tiered', 2), ('libertyreg', 2), ('floral', 2), ('mini', 2), ('gingham', 2), ('midi', 2), ('white', 2), ('later', 2), ('point', 1), ('sur', 1), ('mixed', 1), ('eyelet', 1)]\n"
     ]
    }
   ],
   "source": [
    "clusters = np.loadtxt(out_filename)\n",
    "pp_sec_stage(data, catg, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_names = {0: \"Skirts\", 1: \"Dresses\", 2: \"Dresses\"}\n",
    "update_data(data, catg, catg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt second stage for Catgeory 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229, 1121)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_df = 0.95\n",
    "min_df = 0.005\n",
    "catg = 7\n",
    "num_centers = 3\n",
    "sub_corpus = get_subcorpus(data, catg ) \n",
    "sub_Xvect = tfidf_vectorizer(sub_corpus, max_df = max_df, min_df = min_df)\n",
    "assigned_clusters = kmeans(sub_Xvect,num_centers)\n",
    "#Save assigned clusters to text file\n",
    "out_filename = \"subkmeans_catg_\" + str(catg)\n",
    "np.savetxt(out_filename,assigned_clusters,fmt = '%u', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[('rise', 71), ('leg', 60), ('front', 41), ('detail', 34), ('knee', 23), ('inseam', 22), ('slim', 21), ('hip', 21), ('elastane', 20), ('feature', 19), ('waist', 18), ('classic', 18), ('perfect', 18), ('stretch', 17), ('super', 16)]\n",
      "2.0\n",
      "[('oil', 62), ('extract', 41), ('hand', 40), ('skin', 36), ('acid', 22), ('soft', 22), ('ingredient', 22), ('piece', 22), ('content', 21), ('use', 21), ('care', 21), ('keep', 21), ('looking', 21), ('seed', 21), ('fabric', 19)]\n",
      "0.0\n",
      "[('care', 20), ('album', 17), ('hand', 17), ('mother', 16), ('piece', 14), ('tee', 14), ('model', 13), ('content', 12), ('los', 12), ('angeles', 12), ('vintage', 11), ('vinyl', 11), ('feel', 11), ('feat', 11), ('feature', 10)]\n"
     ]
    }
   ],
   "source": [
    "clusters = np.loadtxt(out_filename)\n",
    "pp_sec_stage(data, catg, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_names = {1: \"Pant\", 0: \"Others\", 2: \"Others\"}\n",
    "update_data(data, catg, catg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt second stage for Catgeory 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173, 350)\n"
     ]
    }
   ],
   "source": [
    "max_df = 0.95\n",
    "min_df = 0.01\n",
    "catg = 4\n",
    "num_centers = 3\n",
    "sub_corpus = get_subcorpus(data, catg ) \n",
    "sub_Xvect = tfidf_vectorizer(sub_corpus, max_df = max_df, min_df = min_df)\n",
    "assigned_clusters = kmeans(sub_Xvect,num_centers)\n",
    "#Save assigned clusters to text file\n",
    "out_filename = \"subkmeans_catg_\" + str(catg)\n",
    "np.savetxt(out_filename,assigned_clusters,fmt = '%u', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "[('pour', 35), ('avec', 33), ('contenu', 25), ('soins', 24), ('taille', 23), ('vous', 18), ('une', 17), ('facile', 16), ('nettoyer', 16), ('extrait', 13), ('composition', 12), ('lavage', 12), ('entretien', 11), ('qui', 11), ('par', 10)]\n",
      "1.0\n",
      "[('pant', 74), ('und', 36), ('mit', 20), ('stretch', 18), ('inhalt', 17), ('pflege', 17), ('footwear', 16), ('suit', 13), ('jean', 13), ('point', 12), ('wool', 12), ('nur', 11), ('abwischen', 10), ('chino', 10), ('sur', 9)]\n",
      "0.0\n",
      "[('jacket', 43), ('field', 8), ('downtown', 7), ('coat', 6), ('tie', 5), ('outerwear', 4), ('waist', 4), ('garment', 3), ('dyed', 3), ('denim', 3), ('perfect', 3), ('rainjacket', 3), ('uncoated', 3), ('blazer', 3), ('chino', 2)]\n"
     ]
    }
   ],
   "source": [
    "clusters = np.loadtxt(out_filename)\n",
    "pp_sec_stage(data, catg, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_names = {1: \"Pant/Some jeans\", 0: \"Others/Jacket\", 2: \"Others\"}\n",
    "update_data(data, catg, catg_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt second stage for Catgeory 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 282)\n"
     ]
    }
   ],
   "source": [
    "max_df = 0.99\n",
    "min_df = 0.0001\n",
    "catg = 3\n",
    "num_centers = 3\n",
    "sub_corpus = get_subcorpus(data, catg ) \n",
    "sub_Xvect = tfidf_vectorizer(sub_corpus, max_df = max_df, min_df = min_df)\n",
    "assigned_clusters = kmeans(sub_Xvect,num_centers)\n",
    "#Save assigned clusters to text file\n",
    "out_filename = \"subkmeans_catg_\" + str(catg)\n",
    "np.savetxt(out_filename,assigned_clusters,fmt = '%u', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[('accessory', 19), ('bag', 14), ('tote', 8), ('straw', 6), ('striped', 4), ('market', 3), ('coin', 3), ('purse', 3), ('hat', 3), ('bracelet', 3), ('watermelon', 2), ('cape', 2), ('canvas', 2), ('backpack', 2), ('embellished', 2)]\n",
      "2.0\n",
      "[('leather', 32), ('heel', 15), ('sole', 14), ('upper', 13), ('lining', 13), ('synthetic', 9), ('closure', 7), ('true', 7), ('measurement', 7), ('taken', 7), ('flat', 6), ('rubber', 6), ('belt', 6), ('boot', 6), ('italian', 5)]\n",
      "1.0\n",
      "[('bag', 16), ('leather', 9), ('pouch', 6), ('italian', 6), ('case', 3), ('mini', 3), ('bucket', 3), ('interior', 3), ('italy', 3), ('signet', 3), ('water', 2), ('resistant', 2), ('wave', 2), ('vegetable', 2), ('tanned', 2)]\n"
     ]
    }
   ],
   "source": [
    "clusters = np.loadtxt(out_filename)\n",
    "pp_sec_stage(data, catg, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "catg_names = {1: \"Bags\", 0: \"Bags\", 2: \"Shoes\"}\n",
    "update_data(data, catg, catg_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved second stage clustering results in final_res.json \n"
     ]
    }
   ],
   "source": [
    "save_sec_stage_res(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
